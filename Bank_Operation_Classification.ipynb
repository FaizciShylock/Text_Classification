{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfied-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we have an excel file \"bank_data.xlsx\" , which contains statements of users in the first column.\n",
    "# The second column consists of the sentences after the statements being stemmed.\n",
    "# The third column holds the corresponding operation names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 : Extracting the data from \"bank_data.xlsx\"\n",
    "# First we need to download the OpenPyXL Module lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "harmful-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import openpyxl as xl\n",
    "from snowballstemmer import TurkishStemmer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twelve-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# To open the workbook  \n",
    "# workbook object is created \n",
    "wb_obj = xl.load_workbook(\"bank_data.xlsx\")\n",
    "\n",
    "# Get workbook active sheet object \n",
    "# from the active attribute \n",
    "sheet_obj = wb_obj.active \n",
    "\n",
    "# Cell objects also have a row, column,  \n",
    "# and coordinate attributes that provide \n",
    "# location information for the cell. \n",
    "\n",
    "# Note: The first row or  \n",
    "# column integer is 1, not 0.\n",
    "\n",
    "# Cell object is created by using  \n",
    "# sheet object's cell() method. \n",
    "cell_obj = sheet_obj.cell(row = 1, column = 1) \n",
    "\n",
    "# Print value of cell object  \n",
    "# using the value attribute \n",
    "print(cell_obj.value, type(cell_obj.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "together-sapphire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4757\n"
     ]
    }
   ],
   "source": [
    "tot_row = sheet_obj.max_row #the total number of rows, including the first line\n",
    "tot_col = sheet_obj.max_column #the total number of col\n",
    "\n",
    "print(tot_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interior-flesh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "\n",
    "for i in range(2, tot_row): \n",
    "    cell_obj_statement = sheet_obj.cell(row = i, column = 2) \n",
    "    cell_obj_intent = sheet_obj.cell(row = i, column = 3)\n",
    "    if type(cell_obj_statement.value) == str:\n",
    "        documents.append((cell_obj_statement.value.lower(), cell_obj_intent.value))\n",
    "        words = word_tokenize(cell_obj_statement.value)\n",
    "        for w in words:\n",
    "            all_words.append(w.lower())\n",
    "            \n",
    "# option_pool = []\n",
    "# for i in range (0,len(documents)-1):\n",
    "#     option_pool.append(documents[:][i][1])\n",
    "\n",
    "# option_set = set(option_pool)    \n",
    "\n",
    "# option_dict = {}\n",
    "\n",
    "# num_opt = 0\n",
    "# for opt in option_set:\n",
    "#     option_dict[opt] = num_opt\n",
    "#     num_opt = num_opt + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "print(len(list(all_words.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "headed-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:200]\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crucial-familiar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conceptual-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)\n",
    "\n",
    "training_set = featuresets[:4000]\n",
    "testing_set = featuresets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-modification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy 59.735099337748345\n",
      "Most Informative Features\n",
      "                aktarmak = True           lower_ : irr    =    411.4 : 1.0\n",
      "                kampanya = True           movie_ : irr    =    301.4 : 1.0\n",
      "                     tür = True           view_a : irr    =    251.6 : 1.0\n",
      "                   neden = True             name : irr    =    214.1 : 1.0\n",
      "                     gün = True           weeken : irr    =    209.0 : 1.0\n",
      "                   hangi = True             team : irr    =    181.5 : 1.0\n",
      "                     atm = True           atm_ca : irr    =    169.9 : 1.0\n",
      "                    vade = True           accoun : irr    =    158.9 : 1.0\n",
      "                 avantaj = True           accoun : irr    =    137.9 : 1.0\n",
      "                bulunmak = True           money_ : irr    =    132.4 : 1.0\n",
      "                  bakiye = True           missin : irr    =    125.2 : 1.0\n",
      "                      tl = True           accoun : irr    =    122.7 : 1.0\n",
      "                   zaman = True           period : irr    =    103.7 : 1.0\n",
      "                      mü = True           weeken : irr    =     99.0 : 1.0\n",
      "                  ödemek = True           paymen : lower_ =     93.3 : 1.0\n",
      "BernoulliNB_classifier accuracy 55.496688741721854\n",
      "SVC_classifier accuracy:  65.82781456953643\n",
      "LinearSVC_classifier accuracy:  67.54966887417218\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy: \", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy: \", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "permanent-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "executed-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers  = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self,features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "funded-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(LinearSVC_classifier)\n",
    "turkStem=TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "valued-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    e_text = \"\"\n",
    "    for w in text.split(\" \"):\n",
    "        w = w.lower()\n",
    "        w = turkStem.stemWord(w) \n",
    "        e_text = e_text + \" \" + w\n",
    "\n",
    "    feats = find_features(e_text)\n",
    "    print(e_text)\n",
    "    \n",
    "    return voted_classifier.classify(feats), voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unauthorized-crime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kart hesap par çekim kısıtla var mı\n",
      "('expense', 1.0)\n",
      " kart hesap otomatik öde talimat oluşturabilir mi\n",
      "('automatic_payment', 1.0)\n",
      " her şey yol gidiyor mu oksi\n",
      "('whats_up', 1.0)\n",
      " kart hesap tanım mı\n",
      "('expense', 1.0)\n",
      " su fatura kart hesap il yatırabilir mi\n",
      "('payment', 1.0)\n",
      " hesap herhangi bir geçerlilik süres var mı\n",
      "('expense', 1.0)\n",
      " haf so kart hesap par yatırabilir mi\n",
      "('cancel', 1.0)\n",
      " kart hesap ar olarak bir hesap açacak\n",
      "('cancel', 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(sentiment(\"Kart hesapta para çekiminde kısıtlama var mı\"))\n",
    "print(sentiment(\"Kart hesaptan otomatik ödeme talimatı oluşturabilir miyim\"))\n",
    "print(sentiment(\"her şey yolunda gidiyor mu oksi\"))\n",
    "print(sentiment(\"Kart hesabını tanımlar mısın\"))\n",
    "print(sentiment(\"su faturamı Kart hesap ile yatırabilir miyim\"))\n",
    "print(sentiment(\"hesabının herhangi bir geçerlilik süresi var mı\"))\n",
    "print(sentiment(\"hafta sonu Kart hesaba para yatırabilir miyim\"))\n",
    "print(sentiment(\"Kart hesaba artı olarak bir hesap açacağım\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-orchestra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-grade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
